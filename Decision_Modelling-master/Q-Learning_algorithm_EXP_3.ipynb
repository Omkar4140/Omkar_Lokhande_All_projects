{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9iE2cgobQI4c","executionInfo":{"status":"ok","timestamp":1716811015111,"user_tz":-330,"elapsed":2632,"user":{"displayName":"Omkar Lokhande","userId":"12029620098751607426"}},"outputId":"c7351e21-31c5-48b0-c334-6de0bf5026ce"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["Learned Q-table:\n","[[ 0.          0.          0.          0.          0.          0.        ]\n"," [-0.38318137 -0.276      -0.28075    -0.28525    -0.276      -1.        ]\n"," [-0.370975   -0.3709875  -0.3669375  -0.366475   -0.43978    -1.        ]\n"," ...\n"," [-0.195      -0.19       -0.195      -0.195      -1.         -1.        ]\n"," [-0.5994565  -0.59583628 -0.66948368 -0.60332431 -1.         -1.9       ]\n"," [-0.195      -0.195      -0.1        -0.1        -1.         -1.        ]]\n","Average Reward: -289.73\n"]}],"source":["import numpy as np\n","import gym\n","\n","# Create the environment\n","env = gym.make('Taxi-v3')\n","\n","# Initialize Q-table with zeros\n","Q = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","# Set hyperparameters\n","alpha = 0.1  # Learning rate\n","gamma = 0.5  # Discount factor\n","epsilon = 0.2  # Exploration rate\n","\n","# Number of episodes\n","episodes = 50\n","\n","# Q-Learning algorithm\n","for _ in range(episodes):\n","    state = env.reset()\n","    done = False\n","    while not done:\n","        # Epsilon-greedy policy\n","        if np.random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample()  # Exploration\n","        else:\n","            action = np.argmax(Q[state])  # Exploitation\n","\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Q-value update\n","        old_q_value = Q[state, action]\n","        next_max = np.max(Q[next_state])\n","        new_q_value = (1 - alpha) * old_q_value + alpha * (reward + gamma * next_max)\n","        Q[state, action] = new_q_value\n","\n","        state = next_state\n","\n","# Print the learned Q-table\n","print(\"Learned Q-table:\")\n","print(Q)\n","\n","# Evaluate the learned policy\n","total_rewards = 0\n","episodes = 100\n","for _ in range(episodes):\n","    state = env.reset()\n","    done = False\n","    while not done:\n","        action = np.argmax(Q[state])\n","        state, reward, done, _ = env.step(action)\n","        total_rewards += reward\n","\n","# Average reward over episodes\n","average_reward = total_rewards / episodes\n","print(\"Average Reward:\", average_reward)\n"]}]}