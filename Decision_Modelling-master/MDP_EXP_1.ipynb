{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wuouANnLoiJ","executionInfo":{"status":"ok","timestamp":1716810812031,"user_tz":-330,"elapsed":10,"user":{"displayName":"Omkar Lokhande","userId":"12029620098751607426"}},"outputId":"50436433-9cc1-41a4-e342-7fbb41d20469"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy (Value Iteration): [1 0 0]\n","Value Function (Value Iteration): [7.36098808 7.78839586 7.74216696]\n","Optimal Policy (Policy Iteration): [1 0 0]\n","Value Function (Policy Iteration): [7.36098834 7.7883961  7.7421672 ]\n","Optimal Policy (Q-Learning): [0 0 0]\n","Q-Table (Q-Learning): [[6.60306568 6.49908673]\n"," [6.87500159 6.53743838]\n"," [6.90107315 5.20307937]]\n"]}],"source":["import numpy as np\n","import random\n","\n","### MDP Algorithms ###\n","\n","def value_iteration(P, R, gamma=0.9, epsilon=1e-6):\n","    n_states, n_actions = R.shape[1], R.shape[0]\n","    V = np.zeros(n_states)\n","    while True:\n","        delta = 0\n","        for s in range(n_states):\n","            v = V[s]\n","            V[s] = max(sum(P[a, s, s1] * (R[a, s] + gamma * V[s1]) for s1 in range(n_states)) for a in range(n_actions))\n","            delta = max(delta, abs(v - V[s]))\n","        if delta < epsilon:\n","            break\n","    policy = np.argmax([[sum(P[a, s, s1] * (R[a, s] + gamma * V[s1]) for s1 in range(n_states)) for a in range(n_actions)] for s in range(n_states)], axis=1)\n","    return policy, V\n","\n","def policy_iteration(P, R, gamma=0.9, epsilon=1e-6):\n","    n_states, n_actions = R.shape[1], R.shape[0]\n","    policy = np.zeros(n_states, dtype=int)\n","    V = np.zeros(n_states)\n","    while True:\n","        while True:\n","            delta = 0\n","            for s in range(n_states):\n","                v = V[s]\n","                V[s] = sum(P[policy[s], s, s1] * (R[policy[s], s] + gamma * V[s1]) for s1 in range(n_states))\n","                delta = max(delta, abs(v - V[s]))\n","            if delta < epsilon:\n","                break\n","        policy_stable = True\n","        for s in range(n_states):\n","            old_action = policy[s]\n","            policy[s] = np.argmax([sum(P[a, s, s1] * (R[a, s] + gamma * V[s1]) for s1 in range(n_states)) for a in range(n_actions)])\n","            if old_action != policy[s]:\n","                policy_stable = False\n","        if policy_stable:\n","            break\n","    return policy, V\n","\n","def q_learning(P, R, gamma=0.9, alpha=0.1, epsilon=0.1, episodes=1000):\n","    n_states, n_actions = R.shape[1], R.shape[0]\n","    Q = np.zeros((n_states, n_actions))\n","    for _ in range(episodes):\n","        state = random.choice(range(n_states))\n","        while True:\n","            if random.uniform(0, 1) < epsilon:\n","                action = random.choice(range(n_actions))\n","            else:\n","                action = np.argmax(Q[state])\n","            next_state = np.argmax(P[action, state])\n","            reward = R[action, state]\n","            best_next_action = np.argmax(Q[next_state])\n","            td_target = reward + gamma * Q[next_state, best_next_action]\n","            td_error = td_target - Q[state, action]\n","            Q[state, action] += alpha * td_error\n","            if state == next_state:\n","                break\n","            state = next_state\n","    policy = np.argmax(Q, axis=1)\n","    return policy, Q\n","\n","### Utility Functions ###\n","\n","def validate_transition_matrix(P):\n","    assert np.allclose(P.sum(axis=2), 1), \"Transition probabilities must sum to 1.\"\n","\n","def validate_reward_matrix(R, P):\n","    assert R.shape == P.shape[:2], \"Reward matrix dimensions must match the transition matrix.\"\n","\n","def generate_random_mdp(n_states, n_actions):\n","    P = np.zeros((n_actions, n_states, n_states))\n","    for a in range(n_actions):\n","        for s in range(n_states):\n","            P[a, s, :] = np.random.dirichlet(np.ones(n_states))\n","    R = np.random.rand(n_actions, n_states)\n","    return P, R\n","\n","### Example Usage ###\n","\n","# Generate a random MDP\n","n_states = 3\n","n_actions = 2\n","P, R = generate_random_mdp(n_states, n_actions)\n","\n","# Validate the MDP\n","validate_transition_matrix(P)\n","validate_reward_matrix(R, P)\n","\n","# Solve the MDP using Value Iteration\n","policy_vi, V_vi = value_iteration(P, R)\n","print(\"Optimal Policy (Value Iteration):\", policy_vi)\n","print(\"Value Function (Value Iteration):\", V_vi)\n","\n","# Solve the MDP using Policy Iteration\n","policy_pi, V_pi = policy_iteration(P, R)\n","print(\"Optimal Policy (Policy Iteration):\", policy_pi)\n","print(\"Value Function (Policy Iteration):\", V_pi)\n","\n","# Solve the MDP using Q-Learning\n","policy_ql, Q_ql = q_learning(P, R)\n","print(\"Optimal Policy (Q-Learning):\", policy_ql)\n","print(\"Q-Table (Q-Learning):\", Q_ql)\n"]}]}