{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XkfVgtK_TX_9","executionInfo":{"status":"ok","timestamp":1716811490910,"user_tz":-330,"elapsed":66039,"user":{"displayName":"Omkar Lokhande","userId":"12029620098751607426"}},"outputId":"de0aed8d-94f8-4a8c-9480-3882cb5239d4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["Episode: 1 Reward: -1377.6548959281724\n","Episode: 2 Reward: -1456.676224918145\n","Episode: 3 Reward: -1390.6151805418854\n","Episode: 4 Reward: -1649.5212680631475\n","Episode: 5 Reward: -1428.1389974280155\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import gym\n","\n","# Actor Model\n","class Actor(tf.keras.Model):\n","    def __init__(self, state_dim, action_dim, action_bound):\n","        super(Actor, self).__init__()\n","        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n","        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n","        self.dense3 = tf.keras.layers.Dense(action_dim, activation='tanh')\n","        self.action_bound = action_bound\n","\n","    def call(self, inputs):\n","        # Reshape the input tensor to have a shape of (batch_size, input_dim)\n","        x = tf.expand_dims(inputs, axis=0)  # Add a batch dimension\n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        x = self.dense3(x)\n","        return tf.squeeze(x, axis=0)  # Remove the added batch dimension\n","\n","\n","# Critic Model\n","class Critic(tf.keras.Model):\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n","        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n","        self.dense3 = tf.keras.layers.Dense(1)\n","\n","    def call(self, inputs):\n","        # Reshape the input tensor to have a shape of (batch_size, input_dim)\n","        x = tf.expand_dims(inputs, axis=0)  # Add a batch dimension\n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        x = self.dense3(x)\n","        return tf.squeeze(x, axis=0)  # Remove the added batch dimension\n","\n","\n","# Actor-Critic Agent\n","class ActorCriticAgent:\n","    def __init__(self, state_dim, action_dim, action_bound, gamma=0.99, actor_lr=0.001, critic_lr=0.001):\n","        self.actor = Actor(state_dim, action_dim, action_bound)\n","        self.critic = Critic()\n","        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n","        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n","        self.gamma = gamma\n","\n","    def get_action(self, state):\n","        return self.actor(tf.convert_to_tensor([state])).numpy()[0]\n","\n","    def train(self, states, actions, rewards, next_states, dones):\n","        # Compute TD targets\n","        next_q_values = self.critic(tf.convert_to_tensor(next_states, dtype=tf.float32))\n","        targets = rewards + (1 - dones) * self.gamma * next_q_values.numpy().flatten()\n","\n","        # Compute advantages\n","        values = self.critic(tf.convert_to_tensor(states, dtype=tf.float32)).numpy().flatten()\n","        advantages = targets - values\n","\n","        # Train actor\n","        with tf.GradientTape() as tape:\n","            actor_actions = self.actor(tf.convert_to_tensor(states, dtype=tf.float32))\n","            actor_loss = -tf.reduce_mean(self.critic(tf.convert_to_tensor(states, dtype=tf.float32)) * actor_actions)\n","        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n","        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n","\n","        # Train critic\n","        with tf.GradientTape() as tape:\n","            critic_values = self.critic(tf.convert_to_tensor(states, dtype=tf.float32))\n","            critic_loss = tf.reduce_mean(tf.square(targets - critic_values))\n","        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n","        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n","\n","# Example Usage\n","env = gym.make('Pendulum-v1')\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","action_bound = env.action_space.high[0]\n","\n","agent = ActorCriticAgent(state_dim, action_dim, action_bound)\n","\n","episodes = 5\n","for episode in range(episodes):\n","    state = env.reset()\n","    episode_reward = 0\n","    while True:\n","        action = agent.get_action(state)\n","        next_state, reward, done, _ = env.step(action)\n","        agent.train(state, action, reward, next_state, done)\n","        episode_reward += reward\n","        state = next_state\n","        if done:\n","            print(\"Episode:\", episode + 1, \"Reward:\", episode_reward)\n","            break\n"]}]}